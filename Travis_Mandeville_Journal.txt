Week 1:
General review of syllabus and class expectations.

Worked briefly with DS9 in looking at an image of M31. Discussed "vignetting" and is source being the way the actual instrument was constructed. This is something that can be overcome and is not necessarily unavoidable. 

Completed a review of Github. Learned how to use it more completely (i.e. having an account and not using it). Can now setup a remote repository on my computer and how to collaborate with files on other contributor's machines. Conflicts seem like something I'm going to be often at odds with. Completed most of the partner activity with Michael.

GitHub Resource: http://swcarpentry.github.io/git-novice/04-changes/index.html (Module: 4,7, and later 5)
 
Couldn't figure out how to "Fork" the 480/Lab-Journal, Michael showed me how to do that on top of activating the upstream function. 

===============================================================================
Week 2:
Began working with JSkyCalc... its impossible.

JSkyCalc breakthrough courtesy of Prof. Tuttle. In order to change the input values in a way that will result in JSkyCalc actually taking hose directions, you must hit enter instead of just tabbing out of the field. An additional note is that in order to select mulitple items in the object list, you first have to open the airmass from the main screen. Once the dedicated oject list box comes up you should then be free to select any single or multiple items. Doing this out of order will result in scrolling through the object list but not being able to select more than one. Also, once you have the list loaded you can simply type in the name of the object your interested in and the specifics will auto-fil. Not a big deal for this assignment (20 objects), but should prove incredibly valuable if I'm ever working with a significantly larger object list. 

Was able to complete most of the NightSky assignment with Michael. JSkyCalc is super usful once you know how to navigate the things that make JSkyCalc... "special" as Prof. Tuttle so generously puts it. One thing to note is that ploting the airmasses of multiple objects with similar RA and Dec, is so redundant that it eventually make the entirety of the plot unreadable. Last, JSkyCalc proficiency note. The moon despite my assumptions is not on the "Planets" button. I know its not a planet but, we seem to grant special privilages to items we strongly depend on (ex. Sun not star, moon not satallite, etc.). This information is on the Seasonal Observatbility table, and notes the phase of the moon (New, Full) for given dates. 

Github progress was a lie!!! I mistakenly cloned the general Astr480/NighSky repo instead of forking it to a repository and then cloning it. Couldn't figure out how to un-clone a link to the repo. So, I decided to just delete the directory on my machine and move all the material needed to submit the assignment into a brand new directory start from scratch. 

===============================================================================
Week 3:
Worked heavily with DS9 looking at FITS files. Some "special" traits of DS9, if you need to create a region, you have to select Edit on the main menu, then Region below than, then you can create a region. Getting information is the first step required to get to other useful information such as Statistics and Histograms among other things. Additionally, this is purely anecdotal, but it seems when selecting the scale for viewing the image, if you are viewing somethig that is decently bright, z-scale seems to be a better setting. Conversely, a low light image seems to be better using the min/max scale. 

Worked on my project proposal. As a result I was forced to use JSkyCalc, and as expected it attempted to frustrate me to the point of snapping my laptop in half. It exentually cooperated and I used it to verify what objects I had identified as Type I Cepheid, will be observable from APO during our observing window. Unfortunately, only 2 of my objects will be viewed at an airmass of less than 1.5. The others are still up and observable, but with an airmass of less than 2. I don't forsee this being a tremendous setback as I am primarily concerned with the period of the luminosity variations. I don't need the optimal airmass to get this information on each star. 

Continued working on the FITS file assignment using DS9. Note: The variability in our flat field is suprisingly "unflat". Also, DS9 had a small moment of specialness that caused the Overscan area on the image, which should not be fractional, to be... fractional. Never underestimate the power of restarting the program as that was the only action that would correct whatever issue was causing this error. For finding information having to do with everything from the Telescope, Observatory, Filter, RA & Dec, even the temperature of the detector during the observation. 

===============================================================================
Week 4:

Worked on the spectra activity. While I was not able to complete it entirely, I did get most of the way through it. The most interesting part by far was using the small spectroscope. Learning how to change the aperature and resolution was incredibly cool. 

I was able to finish the spectra activity on Wednesday. With the remaining time I was able to complete my observing time request for my research project. I needed to double check the magnitudes associated with each of my targets to ensure that not only would they have a usable Apparent Mag, but also the the stars in question would not saturate the detector. I concluded that with the exception of Polaris, most of them should not be an issue. All of my objects will be available between 11:00PM and 1:00AM with airmasses around 1.8 for all objects. This is not necessarily optimal, bt for purposes of my project, this is probably not the end of the world. 

On Friday we got a crash course in reading in data from Meredith (resident Pandas professional). This is very useful for Werk Squad, as we are begining to take apart and analyze all of our identifications, requiring all of the Pandas.  While digesting all of the knowledge presented the most relevant item to someone as 'green' as myself is: help(command). 

Notes:
Isolate a particular column: df['Header'] single column or multiple
Convert value to string:     for col in ['Header_1', 'Header_2', , ]
			     df.astype(str).str.strip()
			     pd.to_numeric(df)  

Built-in plotting:	     df.plot.scatter('Header_1', c='Header_2')

===============================================================================
Week 5:
Monday we spent going over the CCD Characterization project. For the latter half of class we went to work with the CCD to get our data for the project. The CCD was pretty difficult to get to cooperate. Getting an appropriate bias was incredibly wonky due to the fact that we could not set the exposure to zero, but instead the smallest value that would work was only 0.12s. We were able to get about 14 flats. Though the example suggested getting 16, we ran out of time.

Wednesday was largely spent working on coding. Pulling the data out of the FITs file and into a pandas df, was easier than I thought, Meradith's tutorial last Friday was very helpful for this. Subtracting the dataframes proved to be more difficult. I moved on to just attempting to plot the CCD array, but that too was not willing to cooperate... Much more time will be required.

Finally got both the plotting of the CCD array to work, as well as getting the dataframes to subtract from one another. With those main obstacles overcome, I was able to get both the Gain and Read Noise. Also found out that I made a mistake or JSkyCalc betrayed me, i'll conclude the latter has occured. Long story short, none of my objects would have been observable but Professor Tuttle came to the rescue via I_Observe, and new targets were aquired. Observing dates/times for me will be 5/3 (1AM-) and 5/8 (1AM-).

===============================================================================
Week 6:
Monday was spent continuing to work on the CCD Characterization assignment. I worked on calculating and plotting the linearity of our CCD and ran into several issues, which Meredith was able to help with. Assembling the list of Flat fields that we were working with was very difficult as the path to the files was being odd. Tip: If you provide a path to retrieve data from and it comes back empty and you know for a fact that the path provided is correct. Start removing the lowest level directory until at least something is found via the wildcard search, then begin drilling down back to where the data is. Meredith was not able to tell me exactly why ths is occuring, so I'll simplify it and say "Computers are special, mine especially".

Wednesday we went over data reduction techniques for data taken with the ARCSAT telescope. This will be especially helpful for some of the images I got for my research project for the end of the quarter. Though, I will need to suppliment my data with archival data as my second observation window was cancelled due to weather... thanks clouds. Got some additional work done on the CCD Charaterization project. Was able to get the Linearity plot all figured out and complete. 

Friday we went over more data reduction techniques primarily relating to Photometery. This was a life saver as I will need to utilize archival data for my research project as I was not able to observe all of my objects. With respect to the CCD Characterization project I was able to finish the calculations for the Dark current, and have also knocked out the plotting. Began working on the actual write-up, and was instantly reminded of Latex's poor standing in my mind. But we perservere in the name of professionalism, and the fact that Word will crash my computer instantaneously.

================================================================================
Week 7:
Began working on the Reducing Imaging Data assignment. Assembled a composite bias image and used that to reduce noise in some of my actual data from Arcsat. Sent a non-trivial amount of time on reading about and discussing overscan. I think the most stream lined way to describe it is, overscan represents the continuation of the readout process after all th pixels have been readout. Thus the value of the overscan pixels should be equivalent to the bias value of the pixels. 

Wednesday was spent looking at and trying to use photutils which is the actual devil... the documentation is a lifesaver but only to a certain extent. Stackexchane was my only hope, so I spent the majority of the class period reading and searching on that website. 

Friday we spent time going over the expectations for our paper and specifically the outline coming up on Monday. One of the particularly important pieces of info we covered had to do with figure captions. There should be no information that is exclusively within the caption of a figure. Additionally, the figure is only worth including if it is referenced somewhere within the text. References are going to be made much easier if I utilize ADS, since it has the latex format for each paper. I had several references so this will save me a good amount of time.  

================================================================================
Week 8:
Monday was comprised of continuing to work on the Data Reduction lab. Struggled to get the plots to work as python was telling me that the values were stored in a QTable an that object was not callable. Was able to work around this issue by isolation a single column that I was interested as a list. Once that was done it was easy to plot everything I needed. We also reviewed the first draft of the research papers for peers. There are some really interesting projects out there. It was helpful to see the critiques I made on other students papers that I was able to also find and address in my own paper. 

Data Reduction lab was continued on Wednesday. Spent a good amount of time trying to generate a best fit line to my original Aperture Sum, with the Residual Aperture Sum. Once it fitted the slope came out to be roughly what was expeted. I got a value of 1.12, whereas the slope would ideally be 1. Finished up the data reduction lab after battling with the PSF fitting for hours the night pervious. 

Friday was spent working finding usable archival data. The primary challenge is that most data for variable stars simply provideds the period data and not the raw CCD data to reduce. After figuring our how to deal with (Vizier), switch to the Harvard mirror or you get no data. I had to call an audible on the project as the data reduction was going to be the most robust section of my paper, but bad weather left me with only being able to image 2 stars. Instead I will now reduce the data I have, find the magnitude of the star, and consult a light-curve associated with the star and figure out where in the luminosity fluctuation period I made the observation. I will then go on to answer my original question of generating an error function for the Leavitt Law. 

================================================================================
Week 9:
Wednesday I spent most of my time on my research project. I was finally able to get a plot to work that was desribing my periodic variability for EW_Sct. This plot will be similar to other so this is like 7 or 8 birds with  one stone.

Friday was spent going over the SDSS lab. The matching portion of the assignment was not too difficult but getting the data to read in turned out to be a giant pain. Additionally, the columns names are the least instructive things on the planet. 
