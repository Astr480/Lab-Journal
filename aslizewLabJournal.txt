Day 1 and 2
-reviewed basic coding, vocab, for directories, using terminals, unix
-W credit!!!! 10 pages of writing plus graphs etc, plus extra drafts due
-Observing Project-- only optical
-invisible files: ls -A or -a or *.
-rc or profile, configuration files
-github is scary
-VERSION CONTROL, can make comments, connect w people
-Merging, conflicts (too conflicting?)

Day 3:
-Simbad: stellar catalogue
-NED:nasa/ipac extragalactic database
-good for objects, finding other catalogue
-Jskycalc:pln and optimize observing (when/where/how long objects in sky, 1994)
-hard way is Astropy, AstroPLAN dep on astropy but is separate
-iobserve is 30 bucks
-Airmass: atmosphere. how much in the way.  LEAST = STRAIGHT AHEAD, 1.5 or lower
-The Moon impedes obs, its bright (reflected stellar spectra, not good in optical)
PROJECT CONSIDERATIONS: Moon time
-arcsat sassy about nautical Twilight timing, vs astronomical twilight
ACTIVITY:FROM APO, M31, what season is best, on April 16, how many observing hours,
on Dec 31, is the moon in the way/ what is the moons phase, and how many hours can observe on Dec 31

Day 4:
-Partner PROJECT- Navigating the Night Sky with LEXIEEEEEEEEE
-Object type: exoplanets
-Database: NASA
-'&'' for Jupyter notebook leaving kernel open

Day 5:
-continue on partner PROJECT
is observing length corrected? NO, must make array for every day for correct total time
-will continue on friday, where its due, As well as lab journal for week 2 (updated)

Day 6:
-Finished up the partner project,
-I figured out how to do the correct observing length!!!! I had to write out each day but
the astropy time package is super cool.

Day 7:
-Oliver's 481 talk :D
-RESEARCH PROPOSALS~~~~~~~~~~ DUE friday
-end goal= 15 page paper decribing work (observations and analysis done)
-some description (not every command), plots, why sources, also submitting code,
-teams but separate codes and writeups
-scientific paper - introduction, what and why, referenes in intro, tell us something of obects and methods to study,
Data section, observation stuff, weather etc of the Night
-analysis, results, and conclusions (regular flow)
-grading rubric will be on canvas, checklist with points
-May 20 first draft and peer review and tuttle review,
-seconds draft May 29, be much done, some analysis and data spots missing but bulk of outline, sections, references, writing etc done
-June 3- W credit third draft, prose writing,
-measuring dust lanes in spirals, photometry and cmds in clusters, multiple band analysis of local galaxies
-ARCSAT stuffs
-get used to research, try something doable (not a noble prize)
-remote observing, april 29-may 6, may 8-may 11
-Note if using arcsat, a-wing telescope, archive, etc in proposal!!! (how getting data)
-evening arcsat orientation  if many groups, might set people up (have whole nighttime)
-scientific papers, abstracts and conclusions then plots and tables, summary paragraph ideal and what conclusions / main points
-Annual reviews, interesting topics/ project ideas, change w time but general enough to give osme idea of a field, and references and foundations

Day 8:
-Photon road from emission thru space thru opticals thru electronics, all increase noise etc, spectra change
-Broadband photometry, narrow band like faint spectroscopy
-Dust, grav lensing, etc in space
-Intervening materials, smoke, dust in atmosphere and on top of telescopes, weather, sky brightness
-Always-- less photons.
-Lowest airmass is 1
-Kept working on FITS lab, due Monday
-turned in project proposal!

Day 9:
-next week test taking data w CCDs, brief pause to play w SPECTROgraphs cool!HW/reading due next Monday
CCDs:
-Fits flat0012- flat for Arctic isnt rly flat.  Telescope and instrument have vignetting (light blocked), chip bigger than field of view,
-Bias/Overscan is line.  not actual pixels, but where readout amplifiers have gone past and show electronics
- Baffle is where telescope connects to instrument, scatter is in wrong direction (restricts fov)
- Trapped Souls
- Four amplitudes- four times the amt.  Can do >1 amplifiers
-how many counts=??
-usually use logarithmic or zscale (bigger stretch wider range)
-Continuous scale is easier to judge how regions are related (transition regions!!!! dont use rainbow)
-Histograms- different regions are governed by different distributions
-Poisson= How many times does an event occur
-Gaussian = all DF w large samples system.  not always perfect, offset, smoother with more and mroe points
-Data distributions, large area= Gaussian, smaller area- bing bing bong lines
PHOTOns GOVERNED by POISSON (quantized and independent), looking at probability of arrival
-Lots of photons (flat) approached a Gaussian
-Overscan still gaussian (small area tho)
-Variations n CCDs etc change DF,
-Bias = 0 second image.  Look at noise from individual pixels (just readout, not info)
-CCD  detectors and how they change w different wavelengths
-Charge transfers- one/4 number of readout amplifiers (only one at a time), transfer charge from pixel to pixel until amplifier (bucket brigade)
-Use electricity to keep created electrons in single well, but still need to be passed.  Voltage change moves electrons a lil bit! Sometimes lose things methods
-quantum efficiency = photons vs electrons. as time goes by transfer number aint ideal
-Readout CCD, vertical / parallel array, been optimized.  Things get read one at a time, read entire last row then bumps down
-CCD properties, QE, gain, intrinsic stuff, size. While taking data- orientation,
-Linear, up to a point (incr photons, incr electrons LINEARLY, responsiveness in optical- material issue
-Detectors- CMOS technology (detectors in cellphone).  Photon and electron as well as voltage conversion all in pixels each.  this reduces contiguous area of object (electronic take up room, so not taking in light there.  microlenses focuses light onto pixel, fully fill field! cool)
-Types of noise, shot noise, thermal noise/dark current, read noise, and bias level/across every pixel baselevel
-Shot noise= poisson distrib, photons come in windows but theres variation, different counts
-Dark current- fundmtl to material. We cool CCDS from this.  Adds electrons to counts, fcn of time. too cold reduces QE.  DCurrent- more noise over time, so need same length of data
-Read noise is Gaussian, CCD electronics better, set floor number of counts
-use all these characteristics to erase stuff form data!
-LAB w SPECTROgraphs! wowza!

Day 10+11: Finish spectra lab with gas tubes, filters, etc
-Turned in google form for observing project

Day 12: Meredith class, working with files and fits and Astropy
-Using VizieR. Unit and format options, click relevant,  can compute distance, can compute RA, DEC in degrees (with underscore if not frm original data set)
-Table formats: Unlimited rows, tab separated, submit button not for each group, submit and download (asu.tsv, rename as things)
-! in Jupyter notebook-- Shell commands!!!!! (like terminal!!!!!)
-comments in files #, read inc catalogue with Pandas.  ignore comments / ignore a character (comment="#")
-read.csv for any text and column file
-delim whitespace=true, but careful!!! Might mess up.
-sep="\t" separated by tabs !!!!! Nice.
-Drop method-- .drop()[0,1]), inplace=True to replace table instead of make comment
-Dtype warnings- columns have mixed types-- when Pd reads in, auto set data type for each (missing, numbers, spaces not numbers).  Cant infer = object (.dtype_)
-To make all numeric, .astype.  type help[thing] for help
-Use df.apply(function) i.e. Make np.float, do math, etc to apply across all the columns instead of four loops
-.strip() gets rid of strings/whitespace at end, astype string to make all strings, may need for loop
-If index gets crazy, use .loc[:,col]  (: all indices for this particular column), then reassign pd.to_numeric after turned into string to make all numeric
-Dropping na, any or all, inplace true.  (if ANY or ALL in a row are nan, get rid.  Use any if need whole data for each rows)
-Pandas built in plotting, dont need to import matplotlib (.plot, pass col names)
-select types of data. df.query('(colname<#)'), returns subplot.  colormap, vmin, vmax
-astropy tables, use pandas not astropy.  more complete/powerful
-FITS files, import matplotlib
-fits.open('file name')--- need to close file (always open, looking thru file)-- instead,
-with fits.open as f:.,... then once leave tabbed area, file closed
-f.info shows properties, NDU stores mg data and header for extensions, dimensions, cards= nmbr header keywords
-astropy has conversions of ra, dec, to deg, world coords, etc using/from header
-data is a huge array, doesnt make sense yet.
-print median, std, can use astropy for histograms etc (plt.hist(data.ravel(), bins=100)), ravel changes 2d array to 1d array, not inplace so data no change
-zeroith pixel is bottom left, other formats have top left so tell origin as lower
-df.query to pick out certain stars to plot over img
-from astropy.wcs import WCS (world coordinate system), W=WCS('file') to get info frm header, or certain values
-.values when trying to do conversions to prevent pandas frmo freaking outl (makes np arrays, no more panda attachment)
-all_world2pix to object, origin usually 0 but somtimes labeled as 1. in python always 0.  -->> give pixel values of image
-plt scatter of stars over the .imshow
-.sample draws random sample frm a df, like a preview
